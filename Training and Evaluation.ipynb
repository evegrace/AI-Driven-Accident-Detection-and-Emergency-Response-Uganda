{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a5c5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/d5/03/0eade211c504bda872a594f045f98ddcc6caef2b7c63610946845e304d3f/numpy-2.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached numpy-2.3.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Using cached numpy-2.3.2-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.2\n",
      "    Uninstalling numpy-2.3.2:\n",
      "      Successfully uninstalled numpy-2.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\evely\\\\anaconda3\\\\Lib\\\\site-packages\\\\~.mpy.libs\\\\libscipy_openblas64_-860d95b1c38e637ce4509f5fa24fbf2a.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4b2644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 2.3.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\evely\\AppData\\Local\\Temp\\ipykernel_12556\\422586016.py\", line 7, in <module>\n",
      "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 83, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils import _IS_32BIT\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 15, in <module>\n",
      "    from scipy.sparse import issparse\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py\", line 274, in <module>\n",
      "    from ._csr import *\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_recall_fscore_support, confusion_matrix\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compress, islice\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:274\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix, _array_doc_to_matrix\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f83939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 2.3.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\evely\\AppData\\Local\\Temp\\ipykernel_18272\\3189740734.py\", line 7, in <module>\n",
      "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 83, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils import _IS_32BIT\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 15, in <module>\n",
      "    from scipy.sparse import issparse\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py\", line 274, in <module>\n",
      "    from ._csr import *\n",
      "  File \"C:\\Users\\evely\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_recall_fscore_support, confusion_matrix\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compress, islice\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:274\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix, _array_doc_to_matrix\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# from accident_detection_model import AccidentDetectionCNN, AccidentDetectionLoss, VideoPreprocessor\n",
    "\n",
    "class AccidentVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading accident detection video data\n",
    "    Expects data structure:\n",
    "    data_root/\n",
    "    ├── accidents/\n",
    "    │   ├── video1.mp4\n",
    "    │   ├── video2.mp4\n",
    "    │   └── ...\n",
    "    ├── normal/\n",
    "    │   ├── video1.mp4\n",
    "    │   ├── video2.mp4\n",
    "    │   └── ...\n",
    "    └── annotations.json  # Contains frame-level annotations\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, split='train', sequence_length=16, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.split = split\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform or VideoPreprocessor()\n",
    "        \n",
    "        # Load video paths and labels\n",
    "        self.video_paths, self.labels = self._load_data()\n",
    "        \n",
    "        # Load frame-level annotations if available\n",
    "        self.annotations = self._load_annotations()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load video paths and binary labels\"\"\"\n",
    "        video_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        # Load accident videos (label = 1)\n",
    "        accident_dir = os.path.join(self.data_root, 'accidents')\n",
    "        if os.path.exists(accident_dir):\n",
    "            for video_file in os.listdir(accident_dir):\n",
    "                if video_file.endswith(('.mp4', '.avi', '.mov')):\n",
    "                    video_paths.append(os.path.join(accident_dir, video_file))\n",
    "                    labels.append(1)\n",
    "        \n",
    "        # Load normal videos (label = 0)\n",
    "        normal_dir = os.path.join(self.data_root, 'normal')\n",
    "        if os.path.exists(normal_dir):\n",
    "            for video_file in os.listdir(normal_dir):\n",
    "                if video_file.endswith(('.mp4', '.avi', '.mov')):\n",
    "                    video_paths.append(os.path.join(normal_dir, video_file))\n",
    "                    labels.append(0)\n",
    "        \n",
    "        return video_paths, labels\n",
    "    \n",
    "    def _load_annotations(self):\n",
    "        \"\"\"Load detailed frame-level annotations\"\"\"\n",
    "        annotation_path = os.path.join(self.data_root, 'annotations.json')\n",
    "        if os.path.exists(annotation_path):\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _extract_video_frames(self, video_path, max_frames=None):\n",
    "        \"\"\"Extract frames from video\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        \n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            frames.append(frame)\n",
    "            frame_count += 1\n",
    "            \n",
    "            if max_frames and frame_count >= max_frames:\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    def _create_training_sequences(self, frames, label):\n",
    "        \"\"\"Create multiple training sequences from video frames\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        # Create overlapping sequences\n",
    "        step_size = self.sequence_length // 2  # 50% overlap\n",
    "        \n",
    "        for i in range(0, len(frames) - self.sequence_length + 1, step_size):\n",
    "            sequence_frames = frames[i:i + self.sequence_length]\n",
    "            \n",
    "            # Process sequence\n",
    "            processed_sequence = self.transform.create_sequence(sequence_frames)\n",
    "            \n",
    "            # Create motion intensity labels (higher for accident videos)\n",
    "            motion_intensity = 0.8 if label == 1 else np.random.uniform(0.1, 0.4)\n",
    "            \n",
    "            sequences.append({\n",
    "                'frames': processed_sequence,\n",
    "                'anomaly_label': float(label),\n",
    "                'motion_label': motion_intensity\n",
    "            })\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Extract frames from video\n",
    "        frames = self._extract_video_frames(video_path, max_frames=100)\n",
    "        \n",
    "        if len(frames) < self.sequence_length:\n",
    "            # Pad with repeated frames if too short\n",
    "            frames.extend([frames[-1]] * (self.sequence_length - len(frames)))\n",
    "        \n",
    "        # Create training sequences\n",
    "        sequences = self._create_training_sequences(frames, label)\n",
    "        \n",
    "        # Return random sequence for this sample\n",
    "        selected_sequence = sequences[np.random.randint(0, len(sequences))]\n",
    "        \n",
    "        return {\n",
    "            'frames': torch.FloatTensor(selected_sequence['frames']),\n",
    "            'anomaly_labels': torch.FloatTensor([selected_sequence['anomaly_label']]),\n",
    "            'motion_labels': torch.FloatTensor([selected_sequence['motion_label']]),\n",
    "            'video_path': video_path\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccidentDetectionTrainer:\n",
    "    \"\"\"\n",
    "    Training pipeline for accident detection model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Initialize loss function and optimizer\n",
    "        self.loss_fn = AccidentDetectionLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, patience=5, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "        # Setup logging\n",
    "        self.setup_logging()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging for training progress\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(f'training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def train_epoch(self, dataloader):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move data to device\n",
    "            frames = batch['frames'].to(self.device)\n",
    "            targets = {\n",
    "                'anomaly_labels': batch['anomaly_labels'].to(self.device),\n",
    "                'motion_labels': batch['motion_labels'].to(self.device)\n",
    "            }\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(frames)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        return avg_loss\n",
    "    \n",
    "    def validate_epoch(self, dataloader):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "                # Move data to device\n",
    "                frames = batch['frames'].to(self.device)\n",
    "                targets = {\n",
    "                    'anomaly_labels': batch['anomaly_labels'].to(self.device),\n",
    "                    'motion_labels': batch['motion_labels'].to(self.device)\n",
    "                }\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(frames)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                \n",
    "                # Store predictions and ground truth\n",
    "                predictions.extend((outputs['anomaly_score'] > 0.5).cpu().numpy().flatten())\n",
    "                ground_truth.extend(targets['anomaly_labels'].cpu().numpy().flatten())\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = accuracy_score(ground_truth, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            ground_truth, predictions, average='binary'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs=50, save_path='best_model.pth'):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        best_val_accuracy = 0\n",
    "        patience_counter = 0\n",
    "        max_patience = 10\n",
    "        \n",
    "        self.logger.info(f\"Starting training for {num_epochs} epochs\")\n",
    "        self.logger.info(f\"Device: {self.device}\")\n",
    "        self.logger.info(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.logger.info(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = self.validate_epoch(val_loader)\n",
    "            self.val_losses.append(val_metrics['loss'])\n",
    "            self.val_accuracies.append(val_metrics['accuracy'])\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_metrics['loss'])\n",
    "            \n",
    "            # Log metrics\n",
    "            self.logger.info(f\"Train Loss: {train_loss:.4f}\")\n",
    "            self.logger.info(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "            self.logger.info(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "            self.logger.info(f\"Val Precision: {val_metrics['precision']:.4f}\")\n",
    "            self.logger.info(f\"Val Recall: {val_metrics['recall']:.4f}\")\n",
    "            self.logger.info(f\"Val F1: {val_metrics['f1']:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics['accuracy'] > best_val_accuracy:\n",
    "                best_val_accuracy = val_metrics['accuracy']\n",
    "                patience_counter = 0\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_accuracy': val_metrics['accuracy'],\n",
    "                    'train_history': {\n",
    "                        'train_losses': self.train_losses,\n",
    "                        'val_losses': self.val_losses,\n",
    "                        'val_accuracies': self.val_accuracies\n",
    "                    }\n",
    "                }, save_path)\n",
    "                \n",
    "                self.logger.info(f\"New best model saved! Accuracy: {best_val_accuracy:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= max_patience:\n",
    "                self.logger.info(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        \n",
    "        self.logger.info(f\"Training completed. Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        return best_val_accuracy\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.train_losses, label='Train Loss', color='blue')\n",
    "        ax1.plot(self.val_losses, label='Validation Loss', color='red')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(self.val_accuracies, label='Validation Accuracy', color='green')\n",
    "        ax2.set_title('Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccidentDetectionEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluation pipeline for accident detection model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "    \n",
    "    def evaluate_model(self, test_loader, threshold=0.5):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        all_ground_truth = []\n",
    "        all_motion_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                frames = batch['frames'].to(self.device)\n",
    "                labels = batch['anomaly_labels'].cpu().numpy()\n",
    "                \n",
    "                outputs = self.model(frames)\n",
    "                probabilities = outputs['anomaly_score'].cpu().numpy()\n",
    "                predictions = (probabilities > threshold).astype(int)\n",
    "                motion_pred = outputs['motion_intensity'].cpu().numpy()\n",
    "                \n",
    "                all_predictions.extend(predictions.flatten())\n",
    "                all_probabilities.extend(probabilities.flatten())\n",
    "                all_ground_truth.extend(labels.flatten())\n",
    "                all_motion_predictions.extend(motion_pred.flatten())\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(all_ground_truth, all_predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            all_ground_truth, all_predictions, average='binary'\n",
    "        )\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(all_ground_truth, all_predictions)\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm,\n",
    "            'predictions': all_predictions,\n",
    "            'probabilities': all_probabilities,\n",
    "            'ground_truth': all_ground_truth,\n",
    "            'motion_predictions': all_motion_predictions\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_evaluation_results(self, results):\n",
    "        \"\"\"Plot evaluation results\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        sns.heatmap(results['confusion_matrix'], annot=True, fmt='d', ax=ax1, \n",
    "                   xticklabels=['Normal', 'Accident'], \n",
    "                   yticklabels=['Normal', 'Accident'])\n",
    "        ax1.set_title('Confusion Matrix')\n",
    "        ax1.set_xlabel('Predicted')\n",
    "        ax1.set_ylabel('Actual')\n",
    "        \n",
    "        # ROC Curve (simplified)\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        fpr, tpr, _ = roc_curve(results['ground_truth'], results['probabilities'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        ax2.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "        ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        ax2.set_xlabel('False Positive Rate')\n",
    "        ax2.set_ylabel('True Positive Rate')\n",
    "        ax2.set_title('ROC Curve')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Probability Distribution\n",
    "        ax3.hist([p for i, p in enumerate(results['probabilities']) if results['ground_truth'][i] == 0], \n",
    "                alpha=0.7, label='Normal', bins=30)\n",
    "        ax3.hist([p for i, p in enumerate(results['probabilities']) if results['ground_truth'][i] == 1], \n",
    "                alpha=0.7, label='Accident', bins=30)\n",
    "        ax3.set_xlabel('Anomaly Score')\n",
    "        ax3.set_ylabel('Frequency')\n",
    "        ax3.set_title('Score Distribution')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        # Metrics Summary\n",
    "        metrics_text = f\"\"\"\n",
    "        Accuracy: {results['accuracy']:.3f}\n",
    "        Precision: {results['precision']:.3f}\n",
    "        Recall: {results['recall']:.3f}\n",
    "        F1-Score: {results['f1_score']:.3f}\n",
    "        AUC: {roc_auc:.3f}\n",
    "        \"\"\"\n",
    "        ax4.text(0.1, 0.5, metrics_text, fontsize=12, verticalalignment='center')\n",
    "        ax4.axis('off')\n",
    "        ax4.set_title('Performance Metrics')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Data paths (update these to your actual data paths)\n",
    "    DATA_ROOT = \"/path/to/your/accident/dataset\"\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = AccidentVideoDataset(DATA_ROOT, split='train')\n",
    "    val_dataset = AccidentVideoDataset(DATA_ROOT, split='val')\n",
    "    test_dataset = AccidentVideoDataset(DATA_ROOT, split='test')\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AccidentDetectionCNN(num_classes=1, sequence_length=16)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = AccidentDetectionTrainer(model)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting training...\")\n",
    "    best_accuracy = trainer.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=30,\n",
    "        save_path='best_accident_model.pth'\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    checkpoint = torch.load('best_accident_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    evaluator = AccidentDetectionEvaluator(model)\n",
    "    test_results = evaluator.evaluate_model(test_loader)\n",
    "    \n",
    "    # Plot evaluation results\n",
    "    evaluator.plot_evaluation_results(test_results)\n",
    "    \n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {test_results['precision']:.4f}\")\n",
    "    print(f\"Recall: {test_results['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {test_results['f1_score']:.4f}\")\n",
    "    \n",
    "    # Save test results\n",
    "    with open('test_results.json', 'w') as f:\n",
    "        # Convert numpy arrays to lists for JSON serialization\n",
    "        serializable_results = {\n",
    "            'accuracy': float(test_results['accuracy']),\n",
    "            'precision': float(test_results['precision']),\n",
    "            'recall': float(test_results['recall']),\n",
    "            'f1_score': float(test_results['f1_score']),\n",
    "            'confusion_matrix': test_results['confusion_matrix'].tolist()\n",
    "        }\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(\"\\nTraining and evaluation complete!\")\n",
    "    print(\"Results saved to test_results.json\")\n",
    "    print(\"Plots saved as training_history.png and evaluation_results.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
